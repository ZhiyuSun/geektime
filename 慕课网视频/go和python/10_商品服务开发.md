# 商品服务开发

## 需求分析

## 开发过程

### 1. 先建model

利用peewee库做curd
Category表
- name
- parent_category
- level
- is_tab

BaseModel引入
- add_time
- update_time
- is_deleted, 物理删除和逻辑删除
- 重载save方法，调父类方法return super().save(*args, **kwargs)
- 重载delete方法，自己加个默认参数is_permanently=False
- 重载delete_instance，统一加permanently=False
- 小知识：Python里面，类方法，实例也能调用
- 重载select方法，增加is_deleted的查询条件

测试
playhouse提供了PooledMySQLDatabase等连接方式
测试删除，查找等操作

接下来完善其他的model

Brands品牌表
Goods商品表
- 分布式的系统中，一般不会用外键，但初期开发可以用，可以保证一致性，如果后面对性能有影响，可以去掉外键
- 分布式的事务最好的解决方案，就是不要让分布式事务出现
- 商品的图片可以用jsonField存，playhouse里有jsonField
- 详情页图片也用了图片列表存

GoodsCategoryBrand品牌分类
- indexes里指定了联合主键

Banner轮播图

接下来继续生成表

导入数据

### 2. 定义proto

商品接口
- 有一个批量获取商品的接口
品牌分类等接口

这门课的重点在商品接口

### 3. settings,logs

settings

配置nacos

server.py

### 4. handler

写业务逻辑了
go-micro,kratos都有脚本去生成相应的目录结构

运行成功，到consul里面能够看到服务注册上去了

### 5. 写接口

商品列表

orm可以帮你提高效率，但是不能因为使用orm而放弃了解SQL语句

总体看了下，跟我日常开发的内容差不多
先参数判断
再curd
再分页
resp转成proto的结构

### 6. 写测试

一些配置利用配置中心来做

consul.Consul
c.agent.services()
查找目标服务，获取ip和port

### 7. 继续写接口

如果不存在，可以返回empty_pb2.Empty()
context里可以存放错误的返回码和错误的detail

grpc的接口里面都有context，可以在异常时给context补充信息

更新商品接口

商品库存牵涉到库存服务，订单服务也牵涉到库存服务

支付服务

支付成功：确认扣减库存
用户取消：归还库存
订单超时机制：归还库存

两个分布式微服务的数据的同步，会让系统复杂度提高不少
把更多的服务调度的问题，放在go语言里面做，go语言更强一点

写商品分类

service层是为了通用，而不是为了组装业务逻辑

每次改完proto，要重新生成

goods.proto对应一个handler,goods里对应的方法要跟handler里面的写在一起
当然也可以把goods里的接口给拆分开来

goods_srv
- model：定义model
- proto定义接口
- logs
- settings
- handler：业务逻辑
- tests：用于测试
- server.py

### 8. 开始go-web层的开发

goods-web里面不应该依赖user-web的任何服务，否则就不是微服务了

公告的代码抽出来，一定要注意版本管理，另外谁来维护是一个大问题
不抽出来，就各个微服务自己维护

先配router

利用api里面的package去做区别

修改下nacos里面的配置

要理解鸭子类型和struct，不然看代码有点懵，utils.register.consul.register.go
学习go语言的设计理念，这一节好难

api抽了一个公共的部分base.go

老师有一个同步proto文件的脚本，可以同时更新python的和go的proto文件

### 9. 图片服务

单体应用和微服务下的图片存储

阿里云oss 对象存储

中转模式的缺点：
- 100m的文件传输过慢
- 2次文件传输
- gin的节点容易出现瓶颈

前端的缺点：
- 安全问题

阿里的解决方案：
- web端直传：web端跟gin拿签名，然后带着签名传输文件，gin通过id/secret和oss交互
- oss在文件传完后，会像指定的url发起请求，然后gin返回数据给oss
- 最后oss再返回数据给浏览器端

回调的时候，如果是内网ip，oss肯定访问不到
- 可以买阿里的ecs，有公网ip
- 内网穿透技术（续断，花生壳）

常见的内网穿透
续断、花生壳
续断： https://cloud.zhexi.tech/auth/signin

安装客户端，建立隧道（需要配置端口）
oss会跟续断的服务器连接，续断的服务器和本地有个长连接

支付宝支付等功能也涉及到大量的回调，需要掌握本地的内网穿透，一遍调试

把oss服务转成微服务

代码目录
goods-web
- api:里面写各种接口，业务逻辑
- config：里面有各种通用的struct
- forms

### 10. 库存服务

只需要service，供其他服务调用

先设计表结构

inventory
- goods 商品id
- stocks 库存数
- version 版本号  分布式锁的乐观锁

库存历史（可选）
- 用户
- 商品id
- 数量
- 订单id
- 状态（库存预扣除，未出库）

设计proto文件

设置库存
获取库存信息
扣减库存，需要传goodsinfo的列表
库存归还

改nacos

写接口

扣减库存，容易有超卖的问题
什么是事务，执行多个sql是原子性的
因为一并要扣多个商品的库存，引入事务，要么都成功，要么都失败
with db.atomc() as txn：
    有问题
    txn.rollback()

事务只能帮我们完成这一批数据要么成功要么失败
但还是会引起数据不一致，超卖问题——分布式锁

库存归还也会出现竞争，需要加锁

## 11. 分布式锁

面试高频

lock_test.py
多线程下的并发带来的数据不一致的问题

python启动两个线程去做模拟扣除
在运行时增加sleep运行时长

出现超卖情况，库存扣减的并发问题

！！！我做的请假系统也涉及到类似的问题

方法1 让数据库根据自己当前的值更新数据，这个语句能不能处理并发的问题。
update xx set stock=stock-1 where id = xx
能够更新成功，但是有可能两个线程都能买，把商品变成负的
类似两个请假单，同时扣某一年的年假，把年假扣成付的

方法2 加锁
加线程锁
R = threading.Lock()
R.acquire() 
R.release()
这样可以让两个线程变成顺序的
为什么一般的锁在微服务中不能使用？
因为有两台服务器，各自有锁，没法互相影响
要拿第三方的锁（分布式锁）

常见的分布式锁实现方案：
悲观锁
两个服务都在访问同一张表，在数据库上加锁
要锁表，并发性不高
如果并发过高，数据库压力过大，会宕机
悲观锁做起来简单，但是不建议这么做

我们一般会用乐观锁来做
数据库字段加入乐观锁
高并发系统中，如果能不麻烦数据库，就不要麻烦数据库
数据库本身扛高并发能力就不强
乐观锁使用，在更新时同时把version+1,查询条件加上上次查出来的版本
update xx set stock=stock-1, version=version+1 where id = xx and version=version
两个更新语句，第一个来的时候，数据库会锁住，第二条立马用不了
如果第二条更新失败，直接重试
加一个while语句，一旦成功，break，不停的尝试，没有必要一开始就加一把锁
在扛并发能力上比悲观锁要高，乐观锁应用会更多一些
并发越来越高的话，对数据库压力会比较大，因为要一次次的查询

优点：
- 简单
- 不需要额外的组件-维护，mysql的维护比较简单，最合适的才是最好的。保证系统的可用性。多维护一个组件就多一些维护成本

缺点：
- 性能

方法3 基于redis的分布式锁
第一个线程拿lock:goods1
把这个值设置为1
第二个锁，如果是1，就等待，不停的循环，直到可以介入

基于redis的锁，
- 大家尽量自己去完成
- 一定要看懂源码
这是一个面试高频题，连环炮
这是一个非常关键的知识点


实现：redis_lock.py
基础的版本：
安装redis驱动，redis-py
实现acquire和release方法
当并发特别高的时候，两个线程都进入了这个代码：
if not self.redis_client.get(self.name):
    self.redis_client.set(name, 1)
    return True
都按到了锁，出现了问题

如何确保获取锁的原子性：
redis提供了一个命令，setnx，保证了读取和设置key的原子性
修改代码：
self.redis_client.setnx(name, 1)
如果不存在设置并且返回1，否则返回0，这是原子操作
使用setnx确保获取和设置key是原子性


但是这个代码在生产环境中用还是有很多问题
- 互斥性：任意时刻只能有一个客户端拥有锁，不能同时多个客户端获取
- 安全性：锁只能被持有该锁的用户删除，而不能被其他用户删除
- 死锁：获取锁的客户端因为某些原因而宕机，而未能释放锁，其他客户端无法获取此锁，需要有机制来避免该类问题的发生
    - 代码异常，导致无法运行到release
    - 你的当前服务器网络出问题 - 脑裂
    - 断电
- 容错：当部分节点宕机，客户端仍能获取锁或者释放锁

死锁是最严重的问题

如何解决上述问题的发生 - 设置过期时间
set.redis_client.set(name, 1, nx=True, ex=15)
设置过期时间

过期设置会产生新的问题：
- 当前的线程如果在一段时间后没有执行完， 当前的程序没有执行完，然后key过期了
    - 不安全
    - 另一个线程进来以后会将当前的key给删除掉， 另一个线程删除掉了本该属于我设置的值
    - 如果当前的线程没有执行完，那我的这个线程还应该在适当的时候去续租，将过期时间重新设置
        - 应该在什么时候去设置过期  - 15的2/3的时候去续租，也就是运行10s以后去将过期时间重新设置为15s
        - 如果去定时的完成这个续租的过程 - 启动一个线程去完成

续租过期时间-看门狗-java中有一个redission

如何防止我设置的值被其他的线程给删除掉
- 设置key的值为uuid
- release时先做判断，先去出值然后判断当前的值和你自己的lock中的id是否一致，如果一致删除，不一致就报错
但是先get，后delete，由分成了两步，不安全，我们要把它原子化，但是没有函数来把它们合成一步完成
但是redis提供了一个脚本语言，lua
使用lua脚本去完成这个操作，使得该操作原子化

看门狗
- 启动一个线程，定时的去刷新过期时间，这个操作最好也是使用lua脚本来完成

体验别人已经写好的redis的锁

第三方的开源库，python-redis-lock

src/redis_lock/

https://github.com/ionelmc/python-redis-lock

我们写的acquire是阻塞的方式

是否应该去刷新过期时间，不是一定要这样做，这是有风险的。
如果当前的线程一直阻塞，退不出来，就会永远续租，造成死锁。

__enter__ 用来使用with语句

集成redis的分布式锁到库存服务中

配置一个连接池
redis.StrictRedis()

**分布式锁的优缺点**

基于redis的分布式锁

优点：
- 性能高
- 简单
- redis本身使用很频繁，这样的话我们不需要去额外维护

缺点：
- 依赖了第三方组件
- 单机的redis挂掉的可能性相对较高 - redis的cluster redis的sentinel
- redis的cluster的引入会导致刚才的redis的锁会有些问题 - redlock （主要是master和slave同步时，导致的短时间数据一致性问题，网上解决方案，redlock，但是有争议）

其他的分布式锁：
- 基于zookeeper的分布式锁

不是说所有的分布式锁你都需要知道原理：

例子：kazoo/lock.py

为什么不用zookeeper锁，不想引入其他的组件，用现有的组件利于维护